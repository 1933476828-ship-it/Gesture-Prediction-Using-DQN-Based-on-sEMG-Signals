{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83359184",
   "metadata": {},
   "source": [
    "---\n",
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5fea4150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "import matplotlib.pyplot as plt \n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "\n",
    "class ReplayBuffer:\n",
    "    # 'capacity' determines the maximum number of transitions the buffer can store.\n",
    "    def __init__(self, capacity):\n",
    "        # Initialize the buffer with a fixed capacity using a deque.\n",
    "        # When the queue is full (e.g., reaches 10,000 items), adding new data\n",
    "        # automatically pushes out (deletes) the oldest data. \n",
    "        # This implements \"First-In-First-Out\" fixed-capacity management automatically.\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "        \n",
    "    # Stores a single experience (transition) resulting from the agent-environment interaction.\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        # Store a single transition\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # Defines the sampling method. 'batch_size' determines how many transitions \n",
    "    # are used for a single training step (typically 32, 64, or 128).\n",
    "    def sample(self, batch_size):\n",
    "        # Randomly sample a batch of transitions\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        # Unzip the transitions into separate tuples (state, action, etc.)\n",
    "        state, action, reward, next_state, done = zip(*transitions)\n",
    "        # Return the processed data for neural network training.\n",
    "        return np.array(state), action, reward, np.array(next_state), done\n",
    "\n",
    "    # Returns the actual number of transitions currently stored in the buffer.\n",
    "    def size(self):\n",
    "        # Return the current size of the buffer\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df1a194",
   "metadata": {},
   "source": [
    "---\n",
    "## 1D CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9e13a817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn # Neural network module (contains layers)\n",
    "import torch.nn.functional as F # Functional interface (includes activation functions)\n",
    "\n",
    "# Inherits from the PyTorch base class nn.Module\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, num_channels, window_size, num_actions):\n",
    "        '''\n",
    "        num_channels: Number of input data channels (e.g., 12 or 16 sEMG sensors).\n",
    "        window_size:  Length of the input time window (e.g., 50 sampling points). \n",
    "                      Note: In this specific network, it is not directly used to define layer sizes \n",
    "                      because the subsequent pooling layer handles the length dimension.\n",
    "        num_actions:  Number of output actions (e.g., 13 gesture classes), \n",
    "                      corresponding to the number of Q-values output by the DQN.\n",
    "        '''\n",
    "        \n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        # Input shape: (Batch, Channels, Window_Size)\n",
    "        '''\n",
    "        nn.Conv1d:    1D Convolutional layer. This is the core for processing time-series data like sEMG.\n",
    "        in_channels:  Tells the network how many input curves (sensor data) there are.\n",
    "        out_channels: This layer will extract 32 different feature patterns (Feature Maps).\n",
    "        kernel_size:  The field of view size of the kernel; it looks at 3 adjacent time points at a time.\n",
    "        padding:      Zero-padding strategy to ensure the output length after convolution remains \n",
    "                      consistent with the input length (facilitates subsequent calculations).\n",
    "        '''\n",
    "        \n",
    "        # 1D Convolutional Layer 1\n",
    "        self.conv1 = nn.Conv1d(in_channels=num_channels, out_channels=32, kernel_size=3, padding=1)\n",
    "        \n",
    "        # 1D Convolutional Layer 2\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        # We use Global Max Pooling, so the input to FC is just the number of feature maps (64)\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        \n",
    "        # self.fc2: Output layer. The number of output nodes equals the number of actions. \n",
    "        # For DQN, this outputs the Q-value (expected reward) corresponding to each action.\n",
    "        self.fc2 = nn.Linear(128, num_actions) # Output: Q-values for each action\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape comes in as (Batch, Window, Channels) usually, need to transpose for Conv1d\n",
    "        # Target shape for Conv1d: (Batch, Channels, Window)\n",
    "        \n",
    "        # Swap the 1st and 2nd dimensions. Move the 'time axis' to the end \n",
    "        # and the 'channel axis' to the middle to adapt to the convolutional layer.\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # Apply Conv1 -> ReLU\n",
    "        x = F.relu(self.conv1(x))\n",
    "        \n",
    "        # Apply Conv2 -> ReLU\n",
    "        x = F.relu(self.conv2(x))\n",
    "        \n",
    "        # Global Max Pooling over the time dimension\n",
    "        # Reduces (Batch, 64, Window) -> (Batch, 64)\n",
    "        x = torch.max(x, dim=2)[0]\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x) # Returns Q(s, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ac0f8a",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "7ea3cfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np # Added numpy import as it is used in the code\n",
    "\n",
    "class EMGEnvironment:\n",
    "    def __init__(self, file_path, window_size=50, max_samples=None, channels=None): # <--- New parameter\n",
    "            # 1. Load Data\n",
    "            data = scipy.io.loadmat(file_path)\n",
    "            self.emg = data['emg']             \n",
    "            self.labels = data['restimulus']   \n",
    "            \n",
    "            # --- New Code: Truncate the first max_samples data points ---\n",
    "            if max_samples is not None:\n",
    "                self.emg = self.emg[:max_samples]       # Take only the first N rows\n",
    "                self.labels = self.labels[:max_samples] # Truncate labels correspondingly\n",
    "                print(f\"⚠️ Data limited to first {max_samples} samples only (Input size)!\")\n",
    "                \n",
    "            if channels is not None:\n",
    "                # self.emg shape is (time_steps, num_channels)\n",
    "                # [:, channels] means: Keep all time rows, but only keep the columns specified in the channels list\n",
    "                self.emg = self.emg[:, channels]\n",
    "                print(f\"⚠️ Data limited to {channels} channels (Input dimensions)!\")\n",
    "            # ----------------------------------------\n",
    "\n",
    "            # 2. Data Normalization (Calculate mean/std only on the truncated data to avoid future data leakage)\n",
    "            mean = np.mean(self.emg, axis=0)\n",
    "            std = np.std(self.emg, axis=0) + 1e-8 \n",
    "            self.emg = (self.emg - mean) / std\n",
    "            \n",
    "            self.window_size = window_size\n",
    "            self.idx = 0 \n",
    "            self.n_samples = self.emg.shape[0] \n",
    "            self.num_actions = len(np.unique(self.labels)) \n",
    "            self.num_channels = self.emg.shape[1]\n",
    "            \n",
    "            print(f\"⚠️ Data limited to {self.num_actions} labels (Output dimensions)!\")\n",
    "\n",
    "    # This function is called before the start of each Episode to restore the environment to its initial state.\n",
    "    def reset(self):\n",
    "        # Reset pointer to the beginning\n",
    "        self.idx = 0\n",
    "        # Return the first state (first window)\n",
    "        return self.emg[self.idx : self.idx + self.window_size]\n",
    "\n",
    "\n",
    "    # Every time the agent takes an Action, the environment executes this function once.\n",
    "    def step(self, action, step=1):\n",
    "        # 1. Get the ground truth label for the current window\n",
    "        # We take the label of the last timestamp in the window as the target\n",
    "        current_label_idx = self.idx + self.window_size - 1\n",
    "        \n",
    "        # Check if we reached the end of the dataset\n",
    "        if current_label_idx >= self.n_samples - 1:\n",
    "            return np.zeros_like(self.emg[0:self.window_size]), 0, True\n",
    "            \n",
    "        true_label = self.labels[current_label_idx][0]\n",
    "        \n",
    "        # 2. Calculate Reward\n",
    "        # Reward +1 for correct classification, -1 for incorrect\n",
    "        if action == true_label:\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            reward = -1.0\n",
    "            \n",
    "        # 3. Move forward one step\n",
    "        self.idx += step\n",
    "        \n",
    "        # 4. Get Next State\n",
    "        next_state = self.emg[self.idx : self.idx + self.window_size]\n",
    "        \n",
    "        # 5. Check done condition\n",
    "        done = (self.idx + self.window_size >= self.n_samples)\n",
    "        \n",
    "        return next_state, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775d1c55",
   "metadata": {},
   "source": [
    "---\n",
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "96f47a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Data limited to first 5000 samples only (Input size)!\n",
      "⚠️ Data limited to [0, 1] channels (Input dimensions)!\n",
      "⚠️ Data limited to 2 labels (Output dimensions)!\n",
      "Starting training on device: cuda\n",
      "Episode 1/500, Reward: 5.0, Acc: 52.53%, Eps: 0.995\n",
      "Episode 2/500, Reward: -3.0, Acc: 48.48%, Eps: 0.990\n",
      "Episode 3/500, Reward: -1.0, Acc: 49.49%, Eps: 0.985\n",
      "Episode 4/500, Reward: -13.0, Acc: 43.43%, Eps: 0.980\n",
      "Episode 5/500, Reward: -7.0, Acc: 46.46%, Eps: 0.975\n",
      "Episode 6/500, Reward: 9.0, Acc: 54.55%, Eps: 0.970\n",
      "Episode 7/500, Reward: -3.0, Acc: 48.48%, Eps: 0.966\n",
      "Episode 8/500, Reward: 15.0, Acc: 57.58%, Eps: 0.961\n",
      "Episode 9/500, Reward: 1.0, Acc: 50.51%, Eps: 0.956\n",
      "Episode 10/500, Reward: -1.0, Acc: 49.49%, Eps: 0.951\n",
      "Episode 11/500, Reward: 3.0, Acc: 51.52%, Eps: 0.946\n",
      "Episode 12/500, Reward: 5.0, Acc: 52.53%, Eps: 0.942\n",
      "Episode 13/500, Reward: 1.0, Acc: 50.51%, Eps: 0.937\n",
      "Episode 14/500, Reward: 7.0, Acc: 53.54%, Eps: 0.932\n",
      "Episode 15/500, Reward: -1.0, Acc: 49.49%, Eps: 0.928\n",
      "Episode 16/500, Reward: -13.0, Acc: 43.43%, Eps: 0.923\n",
      "Episode 17/500, Reward: -5.0, Acc: 47.47%, Eps: 0.918\n",
      "Episode 18/500, Reward: 11.0, Acc: 55.56%, Eps: 0.914\n",
      "Episode 19/500, Reward: 5.0, Acc: 52.53%, Eps: 0.909\n",
      "Episode 20/500, Reward: 17.0, Acc: 58.59%, Eps: 0.905\n",
      "Episode 21/500, Reward: 19.0, Acc: 59.60%, Eps: 0.900\n",
      "Episode 22/500, Reward: 11.0, Acc: 55.56%, Eps: 0.896\n",
      "Episode 23/500, Reward: 9.0, Acc: 54.55%, Eps: 0.891\n",
      "Episode 24/500, Reward: 21.0, Acc: 60.61%, Eps: 0.887\n",
      "Episode 25/500, Reward: 23.0, Acc: 61.62%, Eps: 0.882\n",
      "Episode 26/500, Reward: 9.0, Acc: 54.55%, Eps: 0.878\n",
      "Episode 27/500, Reward: 9.0, Acc: 54.55%, Eps: 0.873\n",
      "Episode 28/500, Reward: 11.0, Acc: 55.56%, Eps: 0.869\n",
      "Episode 29/500, Reward: 9.0, Acc: 54.55%, Eps: 0.865\n",
      "Episode 30/500, Reward: -1.0, Acc: 49.49%, Eps: 0.860\n",
      "Episode 31/500, Reward: -3.0, Acc: 48.48%, Eps: 0.856\n",
      "Episode 32/500, Reward: 25.0, Acc: 62.63%, Eps: 0.852\n",
      "Episode 33/500, Reward: 7.0, Acc: 53.54%, Eps: 0.848\n",
      "Episode 34/500, Reward: 3.0, Acc: 51.52%, Eps: 0.843\n",
      "Episode 35/500, Reward: 15.0, Acc: 57.58%, Eps: 0.839\n",
      "Episode 36/500, Reward: 23.0, Acc: 61.62%, Eps: 0.835\n",
      "Episode 37/500, Reward: 19.0, Acc: 59.60%, Eps: 0.831\n",
      "Episode 38/500, Reward: 5.0, Acc: 52.53%, Eps: 0.827\n",
      "Episode 39/500, Reward: 15.0, Acc: 57.58%, Eps: 0.822\n",
      "Episode 40/500, Reward: 17.0, Acc: 58.59%, Eps: 0.818\n",
      "Episode 41/500, Reward: 19.0, Acc: 59.60%, Eps: 0.814\n",
      "Episode 42/500, Reward: 19.0, Acc: 59.60%, Eps: 0.810\n",
      "Episode 43/500, Reward: 17.0, Acc: 58.59%, Eps: 0.806\n",
      "Episode 44/500, Reward: 31.0, Acc: 65.66%, Eps: 0.802\n",
      "Episode 45/500, Reward: 19.0, Acc: 59.60%, Eps: 0.798\n",
      "Episode 46/500, Reward: 17.0, Acc: 58.59%, Eps: 0.794\n",
      "Episode 47/500, Reward: 11.0, Acc: 55.56%, Eps: 0.790\n",
      "Episode 48/500, Reward: 11.0, Acc: 55.56%, Eps: 0.786\n",
      "Episode 49/500, Reward: 35.0, Acc: 67.68%, Eps: 0.782\n",
      "Episode 50/500, Reward: 7.0, Acc: 53.54%, Eps: 0.778\n",
      "Episode 51/500, Reward: 15.0, Acc: 57.58%, Eps: 0.774\n",
      "Episode 52/500, Reward: 11.0, Acc: 55.56%, Eps: 0.771\n",
      "Episode 53/500, Reward: 29.0, Acc: 64.65%, Eps: 0.767\n",
      "Episode 54/500, Reward: 35.0, Acc: 67.68%, Eps: 0.763\n",
      "Episode 55/500, Reward: 29.0, Acc: 64.65%, Eps: 0.759\n",
      "Episode 56/500, Reward: 31.0, Acc: 65.66%, Eps: 0.755\n",
      "Episode 57/500, Reward: 23.0, Acc: 61.62%, Eps: 0.751\n",
      "Episode 58/500, Reward: 41.0, Acc: 70.71%, Eps: 0.748\n",
      "Episode 59/500, Reward: 27.0, Acc: 63.64%, Eps: 0.744\n",
      "Episode 60/500, Reward: 17.0, Acc: 58.59%, Eps: 0.740\n",
      "Episode 61/500, Reward: 35.0, Acc: 67.68%, Eps: 0.737\n",
      "Episode 62/500, Reward: 29.0, Acc: 64.65%, Eps: 0.733\n",
      "Episode 63/500, Reward: 25.0, Acc: 62.63%, Eps: 0.729\n",
      "Episode 64/500, Reward: 39.0, Acc: 69.70%, Eps: 0.726\n",
      "Episode 65/500, Reward: 41.0, Acc: 70.71%, Eps: 0.722\n",
      "Episode 66/500, Reward: 19.0, Acc: 59.60%, Eps: 0.718\n",
      "Episode 67/500, Reward: 17.0, Acc: 58.59%, Eps: 0.715\n",
      "Episode 68/500, Reward: 27.0, Acc: 63.64%, Eps: 0.711\n",
      "Episode 69/500, Reward: 17.0, Acc: 58.59%, Eps: 0.708\n",
      "Episode 70/500, Reward: 43.0, Acc: 71.72%, Eps: 0.704\n",
      "Episode 71/500, Reward: 21.0, Acc: 60.61%, Eps: 0.701\n",
      "Episode 72/500, Reward: 19.0, Acc: 59.60%, Eps: 0.697\n",
      "Episode 73/500, Reward: 17.0, Acc: 58.59%, Eps: 0.694\n",
      "Episode 74/500, Reward: 29.0, Acc: 64.65%, Eps: 0.690\n",
      "Episode 75/500, Reward: 33.0, Acc: 66.67%, Eps: 0.687\n",
      "Episode 76/500, Reward: 31.0, Acc: 65.66%, Eps: 0.683\n",
      "Episode 77/500, Reward: 21.0, Acc: 60.61%, Eps: 0.680\n",
      "Episode 78/500, Reward: 43.0, Acc: 71.72%, Eps: 0.676\n",
      "Episode 79/500, Reward: 33.0, Acc: 66.67%, Eps: 0.673\n",
      "Episode 80/500, Reward: 19.0, Acc: 59.60%, Eps: 0.670\n",
      "Episode 81/500, Reward: 27.0, Acc: 63.64%, Eps: 0.666\n",
      "Episode 82/500, Reward: 29.0, Acc: 64.65%, Eps: 0.663\n",
      "Episode 83/500, Reward: 45.0, Acc: 72.73%, Eps: 0.660\n",
      "Episode 84/500, Reward: 45.0, Acc: 72.73%, Eps: 0.656\n",
      "Episode 85/500, Reward: 29.0, Acc: 64.65%, Eps: 0.653\n",
      "Episode 86/500, Reward: 27.0, Acc: 63.64%, Eps: 0.650\n",
      "Episode 87/500, Reward: 37.0, Acc: 68.69%, Eps: 0.647\n",
      "Episode 88/500, Reward: 17.0, Acc: 58.59%, Eps: 0.643\n",
      "Episode 89/500, Reward: 39.0, Acc: 69.70%, Eps: 0.640\n",
      "Episode 90/500, Reward: 45.0, Acc: 72.73%, Eps: 0.637\n",
      "Episode 91/500, Reward: 37.0, Acc: 68.69%, Eps: 0.634\n",
      "Episode 92/500, Reward: 25.0, Acc: 62.63%, Eps: 0.631\n",
      "Episode 93/500, Reward: 35.0, Acc: 67.68%, Eps: 0.627\n",
      "Episode 94/500, Reward: 47.0, Acc: 73.74%, Eps: 0.624\n",
      "Episode 95/500, Reward: 39.0, Acc: 69.70%, Eps: 0.621\n",
      "Episode 96/500, Reward: 55.0, Acc: 77.78%, Eps: 0.618\n",
      "Episode 97/500, Reward: 31.0, Acc: 65.66%, Eps: 0.615\n",
      "Episode 98/500, Reward: 29.0, Acc: 64.65%, Eps: 0.612\n",
      "Episode 99/500, Reward: 37.0, Acc: 68.69%, Eps: 0.609\n",
      "Episode 100/500, Reward: 37.0, Acc: 68.69%, Eps: 0.606\n",
      "Episode 101/500, Reward: 31.0, Acc: 65.66%, Eps: 0.603\n",
      "Episode 102/500, Reward: 37.0, Acc: 68.69%, Eps: 0.600\n",
      "Episode 103/500, Reward: 29.0, Acc: 64.65%, Eps: 0.597\n",
      "Episode 104/500, Reward: 33.0, Acc: 66.67%, Eps: 0.594\n",
      "Episode 105/500, Reward: 37.0, Acc: 68.69%, Eps: 0.591\n",
      "Episode 106/500, Reward: 43.0, Acc: 71.72%, Eps: 0.588\n",
      "Episode 107/500, Reward: 47.0, Acc: 73.74%, Eps: 0.585\n",
      "Episode 108/500, Reward: 35.0, Acc: 67.68%, Eps: 0.582\n",
      "Episode 109/500, Reward: 43.0, Acc: 71.72%, Eps: 0.579\n",
      "Episode 110/500, Reward: 37.0, Acc: 68.69%, Eps: 0.576\n",
      "Episode 111/500, Reward: 43.0, Acc: 71.72%, Eps: 0.573\n",
      "Episode 112/500, Reward: 45.0, Acc: 72.73%, Eps: 0.570\n",
      "Episode 113/500, Reward: 57.0, Acc: 78.79%, Eps: 0.568\n",
      "Episode 114/500, Reward: 51.0, Acc: 75.76%, Eps: 0.565\n",
      "Episode 115/500, Reward: 39.0, Acc: 69.70%, Eps: 0.562\n",
      "Episode 116/500, Reward: 37.0, Acc: 68.69%, Eps: 0.559\n",
      "Episode 117/500, Reward: 47.0, Acc: 73.74%, Eps: 0.556\n",
      "Episode 118/500, Reward: 43.0, Acc: 71.72%, Eps: 0.554\n",
      "Episode 119/500, Reward: 47.0, Acc: 73.74%, Eps: 0.551\n",
      "Episode 120/500, Reward: 41.0, Acc: 70.71%, Eps: 0.548\n",
      "Episode 121/500, Reward: 37.0, Acc: 68.69%, Eps: 0.545\n",
      "Episode 122/500, Reward: 39.0, Acc: 69.70%, Eps: 0.543\n",
      "Episode 123/500, Reward: 49.0, Acc: 74.75%, Eps: 0.540\n",
      "Episode 124/500, Reward: 45.0, Acc: 72.73%, Eps: 0.537\n",
      "Episode 125/500, Reward: 17.0, Acc: 58.59%, Eps: 0.534\n",
      "Episode 126/500, Reward: 55.0, Acc: 77.78%, Eps: 0.532\n",
      "Episode 127/500, Reward: 47.0, Acc: 73.74%, Eps: 0.529\n",
      "Episode 128/500, Reward: 29.0, Acc: 64.65%, Eps: 0.526\n",
      "Episode 129/500, Reward: 43.0, Acc: 71.72%, Eps: 0.524\n",
      "Episode 130/500, Reward: 45.0, Acc: 72.73%, Eps: 0.521\n",
      "Episode 131/500, Reward: 39.0, Acc: 69.70%, Eps: 0.519\n",
      "Episode 132/500, Reward: 53.0, Acc: 76.77%, Eps: 0.516\n",
      "Episode 133/500, Reward: 43.0, Acc: 71.72%, Eps: 0.513\n",
      "Episode 134/500, Reward: 67.0, Acc: 83.84%, Eps: 0.511\n",
      "Episode 135/500, Reward: 41.0, Acc: 70.71%, Eps: 0.508\n",
      "Episode 136/500, Reward: 41.0, Acc: 70.71%, Eps: 0.506\n",
      "Episode 137/500, Reward: 43.0, Acc: 71.72%, Eps: 0.503\n",
      "Episode 138/500, Reward: 57.0, Acc: 78.79%, Eps: 0.501\n",
      "Episode 139/500, Reward: 43.0, Acc: 71.72%, Eps: 0.498\n",
      "Episode 140/500, Reward: 49.0, Acc: 74.75%, Eps: 0.496\n",
      "Episode 141/500, Reward: 57.0, Acc: 78.79%, Eps: 0.493\n",
      "Episode 142/500, Reward: 59.0, Acc: 79.80%, Eps: 0.491\n",
      "Episode 143/500, Reward: 37.0, Acc: 68.69%, Eps: 0.488\n",
      "Episode 144/500, Reward: 49.0, Acc: 74.75%, Eps: 0.486\n",
      "Episode 145/500, Reward: 69.0, Acc: 84.85%, Eps: 0.483\n",
      "Episode 146/500, Reward: 71.0, Acc: 85.86%, Eps: 0.481\n",
      "Episode 147/500, Reward: 57.0, Acc: 78.79%, Eps: 0.479\n",
      "Episode 148/500, Reward: 55.0, Acc: 77.78%, Eps: 0.476\n",
      "Episode 149/500, Reward: 43.0, Acc: 71.72%, Eps: 0.474\n",
      "Episode 150/500, Reward: 57.0, Acc: 78.79%, Eps: 0.471\n",
      "Episode 151/500, Reward: 63.0, Acc: 81.82%, Eps: 0.469\n",
      "Episode 152/500, Reward: 55.0, Acc: 77.78%, Eps: 0.467\n",
      "Episode 153/500, Reward: 39.0, Acc: 69.70%, Eps: 0.464\n",
      "Episode 154/500, Reward: 41.0, Acc: 70.71%, Eps: 0.462\n",
      "Episode 155/500, Reward: 53.0, Acc: 76.77%, Eps: 0.460\n",
      "Episode 156/500, Reward: 39.0, Acc: 69.70%, Eps: 0.458\n",
      "Episode 157/500, Reward: 61.0, Acc: 80.81%, Eps: 0.455\n",
      "Episode 158/500, Reward: 63.0, Acc: 81.82%, Eps: 0.453\n",
      "Episode 159/500, Reward: 57.0, Acc: 78.79%, Eps: 0.451\n",
      "Episode 160/500, Reward: 45.0, Acc: 72.73%, Eps: 0.448\n",
      "Episode 161/500, Reward: 65.0, Acc: 82.83%, Eps: 0.446\n",
      "Episode 162/500, Reward: 61.0, Acc: 80.81%, Eps: 0.444\n",
      "Episode 163/500, Reward: 55.0, Acc: 77.78%, Eps: 0.442\n",
      "Episode 164/500, Reward: 41.0, Acc: 70.71%, Eps: 0.440\n",
      "Episode 165/500, Reward: 39.0, Acc: 69.70%, Eps: 0.437\n",
      "Episode 166/500, Reward: 47.0, Acc: 73.74%, Eps: 0.435\n",
      "Episode 167/500, Reward: 59.0, Acc: 79.80%, Eps: 0.433\n",
      "Episode 168/500, Reward: 53.0, Acc: 76.77%, Eps: 0.431\n",
      "Episode 169/500, Reward: 53.0, Acc: 76.77%, Eps: 0.429\n",
      "Episode 170/500, Reward: 59.0, Acc: 79.80%, Eps: 0.427\n",
      "Episode 171/500, Reward: 55.0, Acc: 77.78%, Eps: 0.424\n",
      "Episode 172/500, Reward: 81.0, Acc: 90.91%, Eps: 0.422\n",
      "Episode 173/500, Reward: 61.0, Acc: 80.81%, Eps: 0.420\n",
      "Episode 174/500, Reward: 71.0, Acc: 85.86%, Eps: 0.418\n",
      "Episode 175/500, Reward: 69.0, Acc: 84.85%, Eps: 0.416\n",
      "Episode 176/500, Reward: 47.0, Acc: 73.74%, Eps: 0.414\n",
      "Episode 177/500, Reward: 49.0, Acc: 74.75%, Eps: 0.412\n",
      "Episode 178/500, Reward: 39.0, Acc: 69.70%, Eps: 0.410\n",
      "Episode 179/500, Reward: 61.0, Acc: 80.81%, Eps: 0.408\n",
      "Episode 180/500, Reward: 63.0, Acc: 81.82%, Eps: 0.406\n",
      "Episode 181/500, Reward: 55.0, Acc: 77.78%, Eps: 0.404\n",
      "Episode 182/500, Reward: 67.0, Acc: 83.84%, Eps: 0.402\n",
      "Episode 183/500, Reward: 29.0, Acc: 64.65%, Eps: 0.400\n",
      "Episode 184/500, Reward: 53.0, Acc: 76.77%, Eps: 0.398\n",
      "Episode 185/500, Reward: 65.0, Acc: 82.83%, Eps: 0.396\n",
      "Episode 186/500, Reward: 53.0, Acc: 76.77%, Eps: 0.394\n",
      "Episode 187/500, Reward: 67.0, Acc: 83.84%, Eps: 0.392\n",
      "Episode 188/500, Reward: 57.0, Acc: 78.79%, Eps: 0.390\n",
      "Episode 189/500, Reward: 61.0, Acc: 80.81%, Eps: 0.388\n",
      "Episode 190/500, Reward: 47.0, Acc: 73.74%, Eps: 0.386\n",
      "Episode 191/500, Reward: 53.0, Acc: 76.77%, Eps: 0.384\n",
      "Episode 192/500, Reward: 57.0, Acc: 78.79%, Eps: 0.382\n",
      "Episode 193/500, Reward: 45.0, Acc: 72.73%, Eps: 0.380\n",
      "Episode 194/500, Reward: 67.0, Acc: 83.84%, Eps: 0.378\n",
      "Episode 195/500, Reward: 61.0, Acc: 80.81%, Eps: 0.376\n",
      "Episode 196/500, Reward: 55.0, Acc: 77.78%, Eps: 0.374\n",
      "Episode 197/500, Reward: 61.0, Acc: 80.81%, Eps: 0.373\n",
      "Episode 198/500, Reward: 63.0, Acc: 81.82%, Eps: 0.371\n",
      "Episode 199/500, Reward: 59.0, Acc: 79.80%, Eps: 0.369\n",
      "Episode 200/500, Reward: 67.0, Acc: 83.84%, Eps: 0.367\n",
      "Episode 201/500, Reward: 59.0, Acc: 79.80%, Eps: 0.365\n",
      "Episode 202/500, Reward: 51.0, Acc: 75.76%, Eps: 0.363\n",
      "Episode 203/500, Reward: 59.0, Acc: 79.80%, Eps: 0.361\n",
      "Episode 204/500, Reward: 57.0, Acc: 78.79%, Eps: 0.360\n",
      "Episode 205/500, Reward: 49.0, Acc: 74.75%, Eps: 0.358\n",
      "Episode 206/500, Reward: 61.0, Acc: 80.81%, Eps: 0.356\n",
      "Episode 207/500, Reward: 65.0, Acc: 82.83%, Eps: 0.354\n",
      "Episode 208/500, Reward: 49.0, Acc: 74.75%, Eps: 0.353\n",
      "Episode 209/500, Reward: 61.0, Acc: 80.81%, Eps: 0.351\n",
      "Episode 210/500, Reward: 65.0, Acc: 82.83%, Eps: 0.349\n",
      "Episode 211/500, Reward: 69.0, Acc: 84.85%, Eps: 0.347\n",
      "Episode 212/500, Reward: 71.0, Acc: 85.86%, Eps: 0.346\n",
      "Episode 213/500, Reward: 65.0, Acc: 82.83%, Eps: 0.344\n",
      "Episode 214/500, Reward: 65.0, Acc: 82.83%, Eps: 0.342\n",
      "Episode 215/500, Reward: 79.0, Acc: 89.90%, Eps: 0.340\n",
      "Episode 216/500, Reward: 73.0, Acc: 86.87%, Eps: 0.339\n",
      "Episode 217/500, Reward: 69.0, Acc: 84.85%, Eps: 0.337\n",
      "Episode 218/500, Reward: 59.0, Acc: 79.80%, Eps: 0.335\n",
      "Episode 219/500, Reward: 67.0, Acc: 83.84%, Eps: 0.334\n",
      "Episode 220/500, Reward: 61.0, Acc: 80.81%, Eps: 0.332\n",
      "Episode 221/500, Reward: 61.0, Acc: 80.81%, Eps: 0.330\n",
      "Episode 222/500, Reward: 79.0, Acc: 89.90%, Eps: 0.329\n",
      "Episode 223/500, Reward: 57.0, Acc: 78.79%, Eps: 0.327\n",
      "Episode 224/500, Reward: 75.0, Acc: 87.88%, Eps: 0.325\n",
      "Episode 225/500, Reward: 65.0, Acc: 82.83%, Eps: 0.324\n",
      "Episode 226/500, Reward: 65.0, Acc: 82.83%, Eps: 0.322\n",
      "Episode 227/500, Reward: 69.0, Acc: 84.85%, Eps: 0.321\n",
      "Episode 228/500, Reward: 61.0, Acc: 80.81%, Eps: 0.319\n",
      "Episode 229/500, Reward: 71.0, Acc: 85.86%, Eps: 0.317\n",
      "Episode 230/500, Reward: 67.0, Acc: 83.84%, Eps: 0.316\n",
      "Episode 231/500, Reward: 55.0, Acc: 77.78%, Eps: 0.314\n",
      "Episode 232/500, Reward: 69.0, Acc: 84.85%, Eps: 0.313\n",
      "Episode 233/500, Reward: 57.0, Acc: 78.79%, Eps: 0.311\n",
      "Episode 234/500, Reward: 57.0, Acc: 78.79%, Eps: 0.309\n",
      "Episode 235/500, Reward: 75.0, Acc: 87.88%, Eps: 0.308\n",
      "Episode 236/500, Reward: 59.0, Acc: 79.80%, Eps: 0.306\n",
      "Episode 237/500, Reward: 87.0, Acc: 93.94%, Eps: 0.305\n",
      "Episode 238/500, Reward: 77.0, Acc: 88.89%, Eps: 0.303\n",
      "Episode 239/500, Reward: 57.0, Acc: 78.79%, Eps: 0.302\n",
      "Episode 240/500, Reward: 57.0, Acc: 78.79%, Eps: 0.300\n",
      "Episode 241/500, Reward: 67.0, Acc: 83.84%, Eps: 0.299\n",
      "Episode 242/500, Reward: 67.0, Acc: 83.84%, Eps: 0.297\n",
      "Episode 243/500, Reward: 71.0, Acc: 85.86%, Eps: 0.296\n",
      "Episode 244/500, Reward: 73.0, Acc: 86.87%, Eps: 0.294\n",
      "Episode 245/500, Reward: 79.0, Acc: 89.90%, Eps: 0.293\n",
      "Episode 246/500, Reward: 73.0, Acc: 86.87%, Eps: 0.291\n",
      "Episode 247/500, Reward: 57.0, Acc: 78.79%, Eps: 0.290\n",
      "Episode 248/500, Reward: 75.0, Acc: 87.88%, Eps: 0.288\n",
      "Episode 249/500, Reward: 67.0, Acc: 83.84%, Eps: 0.287\n",
      "Episode 250/500, Reward: 53.0, Acc: 76.77%, Eps: 0.286\n",
      "Episode 251/500, Reward: 73.0, Acc: 86.87%, Eps: 0.284\n",
      "Episode 252/500, Reward: 69.0, Acc: 84.85%, Eps: 0.283\n",
      "Episode 253/500, Reward: 73.0, Acc: 86.87%, Eps: 0.281\n",
      "Episode 254/500, Reward: 77.0, Acc: 88.89%, Eps: 0.280\n",
      "Episode 255/500, Reward: 75.0, Acc: 87.88%, Eps: 0.279\n",
      "Episode 256/500, Reward: 77.0, Acc: 88.89%, Eps: 0.277\n",
      "Episode 257/500, Reward: 67.0, Acc: 83.84%, Eps: 0.276\n",
      "Episode 258/500, Reward: 59.0, Acc: 79.80%, Eps: 0.274\n",
      "Episode 259/500, Reward: 73.0, Acc: 86.87%, Eps: 0.273\n",
      "Episode 260/500, Reward: 59.0, Acc: 79.80%, Eps: 0.272\n",
      "Episode 261/500, Reward: 79.0, Acc: 89.90%, Eps: 0.270\n",
      "Episode 262/500, Reward: 73.0, Acc: 86.87%, Eps: 0.269\n",
      "Episode 263/500, Reward: 79.0, Acc: 89.90%, Eps: 0.268\n",
      "Episode 264/500, Reward: 67.0, Acc: 83.84%, Eps: 0.266\n",
      "Episode 265/500, Reward: 69.0, Acc: 84.85%, Eps: 0.265\n",
      "Episode 266/500, Reward: 71.0, Acc: 85.86%, Eps: 0.264\n",
      "Episode 267/500, Reward: 61.0, Acc: 80.81%, Eps: 0.262\n",
      "Episode 268/500, Reward: 67.0, Acc: 83.84%, Eps: 0.261\n",
      "Episode 269/500, Reward: 61.0, Acc: 80.81%, Eps: 0.260\n",
      "Episode 270/500, Reward: 73.0, Acc: 86.87%, Eps: 0.258\n",
      "Episode 271/500, Reward: 67.0, Acc: 83.84%, Eps: 0.257\n",
      "Episode 272/500, Reward: 73.0, Acc: 86.87%, Eps: 0.256\n",
      "Episode 273/500, Reward: 93.0, Acc: 96.97%, Eps: 0.255\n",
      "Episode 274/500, Reward: 77.0, Acc: 88.89%, Eps: 0.253\n",
      "Episode 275/500, Reward: 61.0, Acc: 80.81%, Eps: 0.252\n",
      "Episode 276/500, Reward: 77.0, Acc: 88.89%, Eps: 0.251\n",
      "Episode 277/500, Reward: 71.0, Acc: 85.86%, Eps: 0.249\n",
      "Episode 278/500, Reward: 77.0, Acc: 88.89%, Eps: 0.248\n",
      "Episode 279/500, Reward: 85.0, Acc: 92.93%, Eps: 0.247\n",
      "Episode 280/500, Reward: 73.0, Acc: 86.87%, Eps: 0.246\n",
      "Episode 281/500, Reward: 73.0, Acc: 86.87%, Eps: 0.245\n",
      "Episode 282/500, Reward: 81.0, Acc: 90.91%, Eps: 0.243\n",
      "Episode 283/500, Reward: 65.0, Acc: 82.83%, Eps: 0.242\n",
      "Episode 284/500, Reward: 69.0, Acc: 84.85%, Eps: 0.241\n",
      "Episode 285/500, Reward: 79.0, Acc: 89.90%, Eps: 0.240\n",
      "Episode 286/500, Reward: 83.0, Acc: 91.92%, Eps: 0.238\n",
      "Episode 287/500, Reward: 75.0, Acc: 87.88%, Eps: 0.237\n",
      "Episode 288/500, Reward: 73.0, Acc: 86.87%, Eps: 0.236\n",
      "Episode 289/500, Reward: 77.0, Acc: 88.89%, Eps: 0.235\n",
      "Episode 290/500, Reward: 67.0, Acc: 83.84%, Eps: 0.234\n",
      "Episode 291/500, Reward: 75.0, Acc: 87.88%, Eps: 0.233\n",
      "Episode 292/500, Reward: 73.0, Acc: 86.87%, Eps: 0.231\n",
      "Episode 293/500, Reward: 79.0, Acc: 89.90%, Eps: 0.230\n",
      "Episode 294/500, Reward: 81.0, Acc: 90.91%, Eps: 0.229\n",
      "Episode 295/500, Reward: 71.0, Acc: 85.86%, Eps: 0.228\n",
      "Episode 296/500, Reward: 79.0, Acc: 89.90%, Eps: 0.227\n",
      "Episode 297/500, Reward: 83.0, Acc: 91.92%, Eps: 0.226\n",
      "Episode 298/500, Reward: 59.0, Acc: 79.80%, Eps: 0.225\n",
      "Episode 299/500, Reward: 81.0, Acc: 90.91%, Eps: 0.223\n",
      "Episode 300/500, Reward: 77.0, Acc: 88.89%, Eps: 0.222\n",
      "Episode 301/500, Reward: 75.0, Acc: 87.88%, Eps: 0.221\n",
      "Episode 302/500, Reward: 79.0, Acc: 89.90%, Eps: 0.220\n",
      "Episode 303/500, Reward: 87.0, Acc: 93.94%, Eps: 0.219\n",
      "Episode 304/500, Reward: 79.0, Acc: 89.90%, Eps: 0.218\n",
      "Episode 305/500, Reward: 87.0, Acc: 93.94%, Eps: 0.217\n",
      "Episode 306/500, Reward: 87.0, Acc: 93.94%, Eps: 0.216\n",
      "Episode 307/500, Reward: 79.0, Acc: 89.90%, Eps: 0.215\n",
      "Episode 308/500, Reward: 61.0, Acc: 80.81%, Eps: 0.214\n",
      "Episode 309/500, Reward: 75.0, Acc: 87.88%, Eps: 0.212\n",
      "Episode 310/500, Reward: 71.0, Acc: 85.86%, Eps: 0.211\n",
      "Episode 311/500, Reward: 87.0, Acc: 93.94%, Eps: 0.210\n",
      "Episode 312/500, Reward: 71.0, Acc: 85.86%, Eps: 0.209\n",
      "Episode 313/500, Reward: 77.0, Acc: 88.89%, Eps: 0.208\n",
      "Episode 314/500, Reward: 85.0, Acc: 92.93%, Eps: 0.207\n",
      "Episode 315/500, Reward: 77.0, Acc: 88.89%, Eps: 0.206\n",
      "Episode 316/500, Reward: 73.0, Acc: 86.87%, Eps: 0.205\n",
      "Episode 317/500, Reward: 91.0, Acc: 95.96%, Eps: 0.204\n",
      "Episode 318/500, Reward: 79.0, Acc: 89.90%, Eps: 0.203\n",
      "Episode 319/500, Reward: 81.0, Acc: 90.91%, Eps: 0.202\n",
      "Episode 320/500, Reward: 77.0, Acc: 88.89%, Eps: 0.201\n",
      "Episode 321/500, Reward: 79.0, Acc: 89.90%, Eps: 0.200\n",
      "Episode 322/500, Reward: 69.0, Acc: 84.85%, Eps: 0.199\n",
      "Episode 323/500, Reward: 63.0, Acc: 81.82%, Eps: 0.198\n",
      "Episode 324/500, Reward: 79.0, Acc: 89.90%, Eps: 0.197\n",
      "Episode 325/500, Reward: 73.0, Acc: 86.87%, Eps: 0.196\n",
      "Episode 326/500, Reward: 85.0, Acc: 92.93%, Eps: 0.195\n",
      "Episode 327/500, Reward: 77.0, Acc: 88.89%, Eps: 0.194\n",
      "Episode 328/500, Reward: 71.0, Acc: 85.86%, Eps: 0.193\n",
      "Episode 329/500, Reward: 77.0, Acc: 88.89%, Eps: 0.192\n",
      "Episode 330/500, Reward: 81.0, Acc: 90.91%, Eps: 0.191\n",
      "Episode 331/500, Reward: 83.0, Acc: 91.92%, Eps: 0.190\n",
      "Episode 332/500, Reward: 71.0, Acc: 85.86%, Eps: 0.189\n",
      "Episode 333/500, Reward: 85.0, Acc: 92.93%, Eps: 0.188\n",
      "Episode 334/500, Reward: 87.0, Acc: 93.94%, Eps: 0.187\n",
      "Episode 335/500, Reward: 87.0, Acc: 93.94%, Eps: 0.187\n",
      "Episode 336/500, Reward: 57.0, Acc: 78.79%, Eps: 0.186\n",
      "Episode 337/500, Reward: 83.0, Acc: 91.92%, Eps: 0.185\n",
      "Episode 338/500, Reward: 73.0, Acc: 86.87%, Eps: 0.184\n",
      "Episode 339/500, Reward: 81.0, Acc: 90.91%, Eps: 0.183\n",
      "Episode 340/500, Reward: 81.0, Acc: 90.91%, Eps: 0.182\n",
      "Episode 341/500, Reward: 77.0, Acc: 88.89%, Eps: 0.181\n",
      "Episode 342/500, Reward: 57.0, Acc: 78.79%, Eps: 0.180\n",
      "Episode 343/500, Reward: 75.0, Acc: 87.88%, Eps: 0.179\n",
      "Episode 344/500, Reward: 85.0, Acc: 92.93%, Eps: 0.178\n",
      "Episode 345/500, Reward: 73.0, Acc: 86.87%, Eps: 0.177\n",
      "Episode 346/500, Reward: 77.0, Acc: 88.89%, Eps: 0.177\n",
      "Episode 347/500, Reward: 71.0, Acc: 85.86%, Eps: 0.176\n",
      "Episode 348/500, Reward: 87.0, Acc: 93.94%, Eps: 0.175\n",
      "Episode 349/500, Reward: 79.0, Acc: 89.90%, Eps: 0.174\n",
      "Episode 350/500, Reward: 81.0, Acc: 90.91%, Eps: 0.173\n",
      "Episode 351/500, Reward: 87.0, Acc: 93.94%, Eps: 0.172\n",
      "Episode 352/500, Reward: 63.0, Acc: 81.82%, Eps: 0.171\n",
      "Episode 353/500, Reward: 83.0, Acc: 91.92%, Eps: 0.170\n",
      "Episode 354/500, Reward: 83.0, Acc: 91.92%, Eps: 0.170\n",
      "Episode 355/500, Reward: 91.0, Acc: 95.96%, Eps: 0.169\n",
      "Episode 356/500, Reward: 75.0, Acc: 87.88%, Eps: 0.168\n",
      "Episode 357/500, Reward: 81.0, Acc: 90.91%, Eps: 0.167\n",
      "Episode 358/500, Reward: 87.0, Acc: 93.94%, Eps: 0.166\n",
      "Episode 359/500, Reward: 81.0, Acc: 90.91%, Eps: 0.165\n",
      "Episode 360/500, Reward: 79.0, Acc: 89.90%, Eps: 0.165\n",
      "Episode 361/500, Reward: 85.0, Acc: 92.93%, Eps: 0.164\n",
      "Episode 362/500, Reward: 85.0, Acc: 92.93%, Eps: 0.163\n",
      "Episode 363/500, Reward: 85.0, Acc: 92.93%, Eps: 0.162\n",
      "Episode 364/500, Reward: 87.0, Acc: 93.94%, Eps: 0.161\n",
      "Episode 365/500, Reward: 87.0, Acc: 93.94%, Eps: 0.160\n",
      "Episode 366/500, Reward: 83.0, Acc: 91.92%, Eps: 0.160\n",
      "Episode 367/500, Reward: 81.0, Acc: 90.91%, Eps: 0.159\n",
      "Episode 368/500, Reward: 83.0, Acc: 91.92%, Eps: 0.158\n",
      "Episode 369/500, Reward: 81.0, Acc: 90.91%, Eps: 0.157\n",
      "Episode 370/500, Reward: 85.0, Acc: 92.93%, Eps: 0.157\n",
      "Episode 371/500, Reward: 79.0, Acc: 89.90%, Eps: 0.156\n",
      "Episode 372/500, Reward: 95.0, Acc: 97.98%, Eps: 0.155\n",
      "Episode 373/500, Reward: 81.0, Acc: 90.91%, Eps: 0.154\n",
      "Episode 374/500, Reward: 71.0, Acc: 85.86%, Eps: 0.153\n",
      "Episode 375/500, Reward: 85.0, Acc: 92.93%, Eps: 0.153\n",
      "Episode 376/500, Reward: 85.0, Acc: 92.93%, Eps: 0.152\n",
      "Episode 377/500, Reward: 89.0, Acc: 94.95%, Eps: 0.151\n",
      "Episode 378/500, Reward: 89.0, Acc: 94.95%, Eps: 0.150\n",
      "Episode 379/500, Reward: 93.0, Acc: 96.97%, Eps: 0.150\n",
      "Episode 380/500, Reward: 65.0, Acc: 82.83%, Eps: 0.149\n",
      "Episode 381/500, Reward: 77.0, Acc: 88.89%, Eps: 0.148\n",
      "Episode 382/500, Reward: 79.0, Acc: 89.90%, Eps: 0.147\n",
      "Episode 383/500, Reward: 81.0, Acc: 90.91%, Eps: 0.147\n",
      "Episode 384/500, Reward: 87.0, Acc: 93.94%, Eps: 0.146\n",
      "Episode 385/500, Reward: 77.0, Acc: 88.89%, Eps: 0.145\n",
      "Episode 386/500, Reward: 85.0, Acc: 92.93%, Eps: 0.144\n",
      "Episode 387/500, Reward: 79.0, Acc: 89.90%, Eps: 0.144\n",
      "Episode 388/500, Reward: 83.0, Acc: 91.92%, Eps: 0.143\n",
      "Episode 389/500, Reward: 83.0, Acc: 91.92%, Eps: 0.142\n",
      "Episode 390/500, Reward: 89.0, Acc: 94.95%, Eps: 0.142\n",
      "Episode 391/500, Reward: 89.0, Acc: 94.95%, Eps: 0.141\n",
      "Episode 392/500, Reward: 87.0, Acc: 93.94%, Eps: 0.140\n",
      "Episode 393/500, Reward: 71.0, Acc: 85.86%, Eps: 0.139\n",
      "Episode 394/500, Reward: 87.0, Acc: 93.94%, Eps: 0.139\n",
      "Episode 395/500, Reward: 87.0, Acc: 93.94%, Eps: 0.138\n",
      "Episode 396/500, Reward: 69.0, Acc: 84.85%, Eps: 0.137\n",
      "Episode 397/500, Reward: 77.0, Acc: 88.89%, Eps: 0.137\n",
      "Episode 398/500, Reward: 75.0, Acc: 87.88%, Eps: 0.136\n",
      "Episode 399/500, Reward: 79.0, Acc: 89.90%, Eps: 0.135\n",
      "Episode 400/500, Reward: 83.0, Acc: 91.92%, Eps: 0.135\n",
      "Episode 401/500, Reward: 77.0, Acc: 88.89%, Eps: 0.134\n",
      "Episode 402/500, Reward: 89.0, Acc: 94.95%, Eps: 0.133\n",
      "Episode 403/500, Reward: 87.0, Acc: 93.94%, Eps: 0.133\n",
      "Episode 404/500, Reward: 81.0, Acc: 90.91%, Eps: 0.132\n",
      "Episode 405/500, Reward: 95.0, Acc: 97.98%, Eps: 0.131\n",
      "Episode 406/500, Reward: 85.0, Acc: 92.93%, Eps: 0.131\n",
      "Episode 407/500, Reward: 91.0, Acc: 95.96%, Eps: 0.130\n",
      "Episode 408/500, Reward: 87.0, Acc: 93.94%, Eps: 0.129\n",
      "Episode 409/500, Reward: 93.0, Acc: 96.97%, Eps: 0.129\n",
      "Episode 410/500, Reward: 81.0, Acc: 90.91%, Eps: 0.128\n",
      "Episode 411/500, Reward: 89.0, Acc: 94.95%, Eps: 0.127\n",
      "Episode 412/500, Reward: 81.0, Acc: 90.91%, Eps: 0.127\n",
      "Episode 413/500, Reward: 85.0, Acc: 92.93%, Eps: 0.126\n",
      "Episode 414/500, Reward: 89.0, Acc: 94.95%, Eps: 0.126\n",
      "Episode 415/500, Reward: 87.0, Acc: 93.94%, Eps: 0.125\n",
      "Episode 416/500, Reward: 83.0, Acc: 91.92%, Eps: 0.124\n",
      "Episode 417/500, Reward: 77.0, Acc: 88.89%, Eps: 0.124\n",
      "Episode 418/500, Reward: 81.0, Acc: 90.91%, Eps: 0.123\n",
      "Episode 419/500, Reward: 85.0, Acc: 92.93%, Eps: 0.122\n",
      "Episode 420/500, Reward: 93.0, Acc: 96.97%, Eps: 0.122\n",
      "Episode 421/500, Reward: 83.0, Acc: 91.92%, Eps: 0.121\n",
      "Episode 422/500, Reward: 79.0, Acc: 89.90%, Eps: 0.121\n",
      "Episode 423/500, Reward: 85.0, Acc: 92.93%, Eps: 0.120\n",
      "Episode 424/500, Reward: 91.0, Acc: 95.96%, Eps: 0.119\n",
      "Episode 425/500, Reward: 87.0, Acc: 93.94%, Eps: 0.119\n",
      "Episode 426/500, Reward: 91.0, Acc: 95.96%, Eps: 0.118\n",
      "Episode 427/500, Reward: 81.0, Acc: 90.91%, Eps: 0.118\n",
      "Episode 428/500, Reward: 87.0, Acc: 93.94%, Eps: 0.117\n",
      "Episode 429/500, Reward: 93.0, Acc: 96.97%, Eps: 0.116\n",
      "Episode 430/500, Reward: 87.0, Acc: 93.94%, Eps: 0.116\n",
      "Episode 431/500, Reward: 93.0, Acc: 96.97%, Eps: 0.115\n",
      "Episode 432/500, Reward: 77.0, Acc: 88.89%, Eps: 0.115\n",
      "Episode 433/500, Reward: 89.0, Acc: 94.95%, Eps: 0.114\n",
      "Episode 434/500, Reward: 83.0, Acc: 91.92%, Eps: 0.114\n",
      "Episode 435/500, Reward: 67.0, Acc: 83.84%, Eps: 0.113\n",
      "Episode 436/500, Reward: 85.0, Acc: 92.93%, Eps: 0.112\n",
      "Episode 437/500, Reward: 89.0, Acc: 94.95%, Eps: 0.112\n",
      "Episode 438/500, Reward: 85.0, Acc: 92.93%, Eps: 0.111\n",
      "Episode 439/500, Reward: 81.0, Acc: 90.91%, Eps: 0.111\n",
      "Episode 440/500, Reward: 89.0, Acc: 94.95%, Eps: 0.110\n",
      "Episode 441/500, Reward: 73.0, Acc: 86.87%, Eps: 0.110\n",
      "Episode 442/500, Reward: 77.0, Acc: 88.89%, Eps: 0.109\n",
      "Episode 443/500, Reward: 87.0, Acc: 93.94%, Eps: 0.109\n",
      "Episode 444/500, Reward: 91.0, Acc: 95.96%, Eps: 0.108\n",
      "Episode 445/500, Reward: 93.0, Acc: 96.97%, Eps: 0.107\n",
      "Episode 446/500, Reward: 89.0, Acc: 94.95%, Eps: 0.107\n",
      "Episode 447/500, Reward: 71.0, Acc: 85.86%, Eps: 0.106\n",
      "Episode 448/500, Reward: 45.0, Acc: 72.73%, Eps: 0.106\n",
      "Episode 449/500, Reward: 79.0, Acc: 89.90%, Eps: 0.105\n",
      "Episode 450/500, Reward: 85.0, Acc: 92.93%, Eps: 0.105\n",
      "Episode 451/500, Reward: 89.0, Acc: 94.95%, Eps: 0.104\n",
      "Episode 452/500, Reward: 87.0, Acc: 93.94%, Eps: 0.104\n",
      "Episode 453/500, Reward: 81.0, Acc: 90.91%, Eps: 0.103\n",
      "Episode 454/500, Reward: 87.0, Acc: 93.94%, Eps: 0.103\n",
      "Episode 455/500, Reward: 87.0, Acc: 93.94%, Eps: 0.102\n",
      "Episode 456/500, Reward: 87.0, Acc: 93.94%, Eps: 0.102\n",
      "Episode 457/500, Reward: 89.0, Acc: 94.95%, Eps: 0.101\n",
      "Episode 458/500, Reward: 89.0, Acc: 94.95%, Eps: 0.101\n",
      "Episode 459/500, Reward: 93.0, Acc: 96.97%, Eps: 0.100\n",
      "Episode 460/500, Reward: 85.0, Acc: 92.93%, Eps: 0.100\n",
      "Episode 461/500, Reward: 97.0, Acc: 98.99%, Eps: 0.099\n",
      "Episode 462/500, Reward: 89.0, Acc: 94.95%, Eps: 0.099\n",
      "Episode 463/500, Reward: 79.0, Acc: 89.90%, Eps: 0.098\n",
      "Episode 464/500, Reward: 87.0, Acc: 93.94%, Eps: 0.098\n",
      "Episode 465/500, Reward: 91.0, Acc: 95.96%, Eps: 0.097\n",
      "Episode 466/500, Reward: 83.0, Acc: 91.92%, Eps: 0.097\n",
      "Episode 467/500, Reward: 93.0, Acc: 96.97%, Eps: 0.096\n",
      "Episode 468/500, Reward: 79.0, Acc: 89.90%, Eps: 0.096\n",
      "Episode 469/500, Reward: 81.0, Acc: 90.91%, Eps: 0.095\n",
      "Episode 470/500, Reward: 87.0, Acc: 93.94%, Eps: 0.095\n",
      "Episode 471/500, Reward: 87.0, Acc: 93.94%, Eps: 0.094\n",
      "Episode 472/500, Reward: 79.0, Acc: 89.90%, Eps: 0.094\n",
      "Episode 473/500, Reward: 77.0, Acc: 88.89%, Eps: 0.093\n",
      "Episode 474/500, Reward: 71.0, Acc: 85.86%, Eps: 0.093\n",
      "Episode 475/500, Reward: 89.0, Acc: 94.95%, Eps: 0.092\n",
      "Episode 476/500, Reward: 87.0, Acc: 93.94%, Eps: 0.092\n",
      "Episode 477/500, Reward: 89.0, Acc: 94.95%, Eps: 0.092\n",
      "Episode 478/500, Reward: 93.0, Acc: 96.97%, Eps: 0.091\n",
      "Episode 479/500, Reward: 97.0, Acc: 98.99%, Eps: 0.091\n",
      "Episode 480/500, Reward: 91.0, Acc: 95.96%, Eps: 0.090\n",
      "Episode 481/500, Reward: 89.0, Acc: 94.95%, Eps: 0.090\n",
      "Episode 482/500, Reward: 93.0, Acc: 96.97%, Eps: 0.089\n",
      "Episode 483/500, Reward: 95.0, Acc: 97.98%, Eps: 0.089\n",
      "Episode 484/500, Reward: 91.0, Acc: 95.96%, Eps: 0.088\n",
      "Episode 485/500, Reward: 85.0, Acc: 92.93%, Eps: 0.088\n",
      "Episode 486/500, Reward: 85.0, Acc: 92.93%, Eps: 0.088\n",
      "Episode 487/500, Reward: 93.0, Acc: 96.97%, Eps: 0.087\n",
      "Episode 488/500, Reward: 87.0, Acc: 93.94%, Eps: 0.087\n",
      "Episode 489/500, Reward: 97.0, Acc: 98.99%, Eps: 0.086\n",
      "Episode 490/500, Reward: 85.0, Acc: 92.93%, Eps: 0.086\n",
      "Episode 491/500, Reward: 93.0, Acc: 96.97%, Eps: 0.085\n",
      "Episode 492/500, Reward: 91.0, Acc: 95.96%, Eps: 0.085\n",
      "Episode 493/500, Reward: 79.0, Acc: 89.90%, Eps: 0.084\n",
      "Episode 494/500, Reward: 95.0, Acc: 97.98%, Eps: 0.084\n",
      "Episode 495/500, Reward: 95.0, Acc: 97.98%, Eps: 0.084\n",
      "Episode 496/500, Reward: 89.0, Acc: 94.95%, Eps: 0.083\n",
      "Episode 497/500, Reward: 93.0, Acc: 96.97%, Eps: 0.083\n",
      "Episode 498/500, Reward: 83.0, Acc: 91.92%, Eps: 0.082\n",
      "Episode 499/500, Reward: 89.0, Acc: 94.95%, Eps: 0.082\n",
      "Episode 500/500, Reward: 57.0, Acc: 78.79%, Eps: 0.082\n",
      "Training Finished. Generating reports...\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # Library for plotting graphs\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "\n",
    "# --- Hyperparameters & Configuration ---\n",
    "# stored in a dictionary for easy writing to CSV later\n",
    "# These parameters control the training physics and network behavior.\n",
    "PARAMS = {\n",
    "    'FILE_PATH': 's1/S1_E1_A1.mat',   # Path to the EMG data file\n",
    "    'WINDOW_SIZE': 50,                # Length of the time series window (state size)\n",
    "    'BATCH_SIZE': 64,                # Number of samples to train on in one iteration\n",
    "    'LR': 0.001,                      # Learning Rate: step size for gradient descent\n",
    "    'GAMMA': 0.995,                    # Discount Factor: Importance of future rewards (0=short-sighted, 1=far-sighted)\n",
    "    'EPSILON_START': 1.0,             # Initial exploration rate (100% random actions at start)\n",
    "    'EPSILON_DECAY': 0.995,           # Decay rate: How fast exploration reduces per episode\n",
    "    'EPSILON_MIN': 0.01,              # Minimum exploration rate (always keep 1% chance to explore)\n",
    "    'BUFFER_CAPACITY': 10000,         # Max size of Replay Buffer (First-In-First-Out)\n",
    "    'NUM_EPISODES': 500,               # Total number of training episodes (iterations)\n",
    "    'MAX_SAMPLES': 5000,             # Limit input data size (truncation)\n",
    "    'CHANNELS': list(range(2)),       # Active EMG sensor channels to use\n",
    "    'STEP': 50 ,                      # Stride: How many time steps to move window forward\n",
    "    'Output_dimensions' : 2\n",
    "}\n",
    "\n",
    "# Unpack a mutable variable for use in the loop\n",
    "EPSILON = PARAMS['EPSILON_START']\n",
    "\n",
    "# --- Initialization ---\n",
    "# Select GPU if available, otherwise fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize Environment, Network, Optimizer, and Buffer\n",
    "# 1. Environment: Custom wrapper to handle EMG data as an RL environment\n",
    "env = EMGEnvironment(PARAMS['FILE_PATH'], PARAMS['WINDOW_SIZE'], PARAMS['MAX_SAMPLES'], PARAMS['CHANNELS'])\n",
    "\n",
    "# 2. Q-Network: The Neural Network approximating the Q-function Q(s, a)\n",
    "q_net = QNetwork(env.num_channels, PARAMS['WINDOW_SIZE'], env.num_actions).to(device)\n",
    "\n",
    "# 3. Optimizer: Adam is used to update network weights based on the learning rate\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=PARAMS['LR'])\n",
    "\n",
    "# 4. Loss Function: Mean Squared Error (MSE) measures the difference between prediction and target\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# 5. Replay Buffer: Stores past experiences to break temporal correlation during training\n",
    "buffer = ReplayBuffer(PARAMS['BUFFER_CAPACITY'])\n",
    "\n",
    "# --- History Lists (For Logging) ---\n",
    "history_rewards = []\n",
    "history_accuracy = []\n",
    "history_epsilon = []\n",
    "history_episodes = []\n",
    "\n",
    "# --- Main Training Loop ---\n",
    "print(f\"Starting training on device: {device}\")\n",
    "start_time = time.time()\n",
    "\n",
    "for episode in range(PARAMS['NUM_EPISODES']):\n",
    "    # Reset environment to start state at the beginning of each episode\n",
    "    state = env.reset()\n",
    "    step_count = 0\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    # Loop until the episode ends (dataset exhausted)\n",
    "    while not done:\n",
    "        # -------------------------------------------------------\n",
    "        # 1. Action Selection (Epsilon-Greedy Strategy)\n",
    "        # -------------------------------------------------------\n",
    "        # Explore: With probability EPSILON, choose a random action\n",
    "        if np.random.rand() < EPSILON:\n",
    "            action = np.random.randint(0, env.num_actions)\n",
    "        # Exploit: Otherwise, ask the Neural Network for the best action\n",
    "        else:\n",
    "            with torch.no_grad(): # Disable gradient calc for inference (saves memory/speed)\n",
    "                # Convert state to tensor and add batch dimension: (C, W) -> (1, C, W)\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                # Get Q-values for all actions\n",
    "                q_values = q_net(state_tensor)\n",
    "                # Select the action index with the highest Q-value\n",
    "                action = torch.argmax(q_values).item()\n",
    "        \n",
    "        # -------------------------------------------------------\n",
    "        # 2. Environment Interaction\n",
    "        # -------------------------------------------------------\n",
    "        # Execute the action, receive feedback (reward) and new state\n",
    "        next_state, reward, done = env.step(action, PARAMS['STEP'])\n",
    "        \n",
    "        step_count += 1\n",
    "        \n",
    "        # Store the transition tuple in the Replay Buffer\n",
    "        buffer.push(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Update current state to next state for the next iteration\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        # -------------------------------------------------------\n",
    "        # 3. Network Training (Experience Replay)\n",
    "        # -------------------------------------------------------\n",
    "        # Start training only when we have enough data in the buffer\n",
    "        if buffer.size() > PARAMS['BATCH_SIZE']:\n",
    "            # Randomly sample a batch of transitions\n",
    "            b_states, b_actions, b_rewards, b_next_states, b_dones = buffer.sample(PARAMS['BATCH_SIZE'])\n",
    "            \n",
    "            # Convert numpy arrays to PyTorch tensors and move to GPU/CPU\n",
    "            b_states = torch.FloatTensor(b_states).to(device)\n",
    "            b_actions = torch.LongTensor(b_actions).unsqueeze(1).to(device) # Shape: (batch, 1)\n",
    "            b_rewards = torch.FloatTensor(b_rewards).to(device)\n",
    "            b_next_states = torch.FloatTensor(b_next_states).to(device)\n",
    "            b_dones = torch.FloatTensor(b_dones).to(device)\n",
    "            \n",
    "            # --- Compute Q_current (Predicted Q) ---\n",
    "            # Pass batch states through network. gather(1, b_actions) selects only \n",
    "            # the Q-value corresponding to the action that was actually taken.\n",
    "            q_current = q_net(b_states).gather(1, b_actions).squeeze(1)\n",
    "            \n",
    "            # --- Compute Q_target (Bellman Equation) ---\n",
    "            # We do not want to update gradients for the target calculation\n",
    "            with torch.no_grad():\n",
    "                # Find the max Q-value for the *next* state (Logic: best possible future)\n",
    "                q_next = q_net(b_next_states).max(1)[0]\n",
    "                \n",
    "                # Formula: Target = Reward + Gamma * Max(Q_next)\n",
    "                # If done is True (1), the future reward is 0.\n",
    "                q_target = b_rewards + (PARAMS['GAMMA'] * q_next * (1 - b_dones))\n",
    "            \n",
    "            # --- Optimization Step ---\n",
    "            # Calculate loss between Predicted Q and Target Q\n",
    "            loss = loss_fn(q_current, q_target)\n",
    "            \n",
    "            # Backpropagation: Clear old gradients, compute new gradients, update weights\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    # -------------------------------------------------------\n",
    "    # 4. End of Episode Updates & Logging\n",
    "    # -------------------------------------------------------\n",
    "    # Decay Epsilon: Reduce exploration rate exponentially, but stop at EPSILON_MIN\n",
    "    EPSILON = max(PARAMS['EPSILON_MIN'], EPSILON * PARAMS['EPSILON_DECAY'])\n",
    "    \n",
    "    # Calculate accuracy based on rewards (Assuming Reward is +1 for correct, -1 for wrong)\n",
    "    # Formula converts range [-step_count, +step_count] to [0, 100]\n",
    "    accuracy = (total_reward + step_count) / (2 * step_count) * 100\n",
    "    \n",
    "    # --- Store data for this episode ---\n",
    "    history_episodes.append(episode + 1)\n",
    "    history_rewards.append(total_reward)\n",
    "    history_accuracy.append(accuracy)\n",
    "    history_epsilon.append(EPSILON)\n",
    "    \n",
    "    PARAMS['Output_dimensions'] = env.num_actions\n",
    "    \n",
    "    \n",
    "    print(f\"Episode {episode+1}/{PARAMS['NUM_EPISODES']}, Reward: {total_reward:.1f}, Acc: {accuracy:.2f}%, Eps: {EPSILON:.3f}\")\n",
    "\n",
    "print(\"Training Finished. Generating reports...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35915b63",
   "metadata": {},
   "source": [
    "---\n",
    "## Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "8576c0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Log saved to: data/training_log_20260112-085544.csv\n",
      "PDF Report saved to: graph/training_plot_20260112-085544.pdf\n"
     ]
    }
   ],
   "source": [
    "# --- Save Results Logic ---\n",
    "# Create a timestamp string to ensure unique filenames\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "csv_filename = f\"data/training_log_{timestamp}.csv\"\n",
    "pdf_filename = f\"graph/training_plot_{timestamp}.pdf\"\n",
    "\n",
    "# 1. Save CSV (Hyperparameters + Training Data)\n",
    "with open(csv_filename, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    # Section 1: Hyperparameters\n",
    "    writer.writerow([\"--- Hyperparameters ---\"])\n",
    "    for key, value in PARAMS.items():\n",
    "        writer.writerow([key, value])\n",
    "    \n",
    "    writer.writerow([]) # Empty line for separation\n",
    "    \n",
    "    # Section 2: Training Data\n",
    "    writer.writerow([\"--- Training Data ---\"])\n",
    "    writer.writerow([\"Episode\", \"Total Reward\", \"Accuracy (%)\", \"Epsilon\"])\n",
    "    \n",
    "    # Zip lists together to write row by row\n",
    "    rows = zip(history_episodes, history_rewards, history_accuracy, history_epsilon)\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"CSV Log saved to: {csv_filename}\")\n",
    "\n",
    "# 2. Save PDF Plot\n",
    "plt.figure(figsize=(10, 8)) # Set figure size (Width, Height)\n",
    "\n",
    "# Subplot 1: Accuracy\n",
    "plt.subplot(2, 1, 1) # (Rows, Cols, Index)\n",
    "plt.plot(history_episodes, history_accuracy, label='Accuracy', color='blue', linewidth=2)\n",
    "plt.title('Training Accuracy per Episode')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 2: Total Reward\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(history_episodes, history_rewards, label='Total Reward', color='green', linewidth=2)\n",
    "plt.title('Total Reward per Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and close\n",
    "plt.savefig(pdf_filename)\n",
    "plt.close()\n",
    "\n",
    "print(f\"PDF Report saved to: {pdf_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
